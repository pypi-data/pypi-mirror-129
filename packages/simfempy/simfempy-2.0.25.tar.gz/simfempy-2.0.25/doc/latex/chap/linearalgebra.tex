% !TEX root = ../simfempy.tex
%
%==========================================
\section{Linear Algebra}\label{sec:}
%==========================================
%
%
%-------------------------------------------------------------------------
\subsection{Saddle-point systems}\label{subsec:}
%-------------------------------------------------------------------------
%
Let $(X,\scp{\cdot}{\cdot})$ be a Hilbert space and 
%
\begin{align*}
X_c := \SetDef{x\in X}{Bx = c}.
\end{align*}
%
We consider the minimization problem with symmetric $A\in \mathcal L(X,X)$ and $B\in \mathcal L(X,Y)$
%
\begin{equation}\label{eq:}
\inf\SetDef{\frac12\scp{Ax}{x}-\scp{b}{x}}{x \in X_c}
\end{equation}
%
With the Lagrangian $L(x,y):= J(x)+ \scp{Bx-c}{y}$, $J(x) :=\frac12\scp{Ax}{x}-\scp{b}{x} $ we get the saddle-point system
%
\begin{equation}\label{eq:SaddlePoint}
\begin{bmatrix}
A & \transpose{B}\\
B & 0
\end{bmatrix}
\begin{bmatrix}
x\\y
\end{bmatrix}
=
\begin{bmatrix}
b\\c
\end{bmatrix}
\end{equation}
%
%
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsubsection{Solution by the kernel method}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
Since $X_0=\ker B$, we may write $X= X_0\oplus X_1$ and split the vectors and matrices accordingly.
%
\begin{equation}\label{eq:SaddlePointKernel}
\begin{bmatrix}
A_{00} & A_{01} & \transpose{B}_0\\
A_{10} & A_{11} & \transpose{B}_1\\
B_0 & B_1 & 0
\end{bmatrix}
\begin{bmatrix}
x_0\\x_1\\y
\end{bmatrix}
=
\begin{bmatrix}
b_0\\b_1\\c
\end{bmatrix}
\end{equation}
%
But by definition we have $B_0=0$ and we can rewrite (\ref{eq:SaddlePointKernel}) as
%
\begin{equation}\label{eq:SaddlePointKernel}
\begin{bmatrix}
A_{00} & A_{01} & 0\\
0 & B_1 & 0\\
A_{10} & A_{11} & \transpose{B}_1\\
\end{bmatrix}
\begin{bmatrix}
x_0\\x_1\\y
\end{bmatrix}
=
\begin{bmatrix}
b_0\\c\\b_1
\end{bmatrix}
\end{equation}
%
The system is well-posed, since $B_1$ is invertible.
%
%
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsubsection{Penalty and augmented Lagrangian method}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
The penalty method is the unconstrained minimization of $J_{\eps}(x):= J(x) + \frac{1}{2\eps}\norm{Bx-c}^2_{M^{-1}}$
and leads to the penalty system
%
\begin{equation}\label{eq:SaddlePointPenalty}
(A + \frac{1}{\eps} \transpose{B}M^{-1}B) x = b + \frac{1}{\eps} \transpose{B}M^{-1}c.
\end{equation}
%
It is equivalent to the modified system
%
\begin{align*}
\begin{bmatrix}
A & \transpose{B}\\
B & -\eps M
\end{bmatrix}
\begin{bmatrix}
x\\y
\end{bmatrix}
=
\begin{bmatrix}
b\\c
\end{bmatrix}
\end{align*}
%
The augmented Lagrangian method uses the Lagrangian $L_r(x,y):= J(x) + \frac{r}{2}\norm{Bx-c}^2_{M^{-1}} + \scp{Bx-c}{y}$ and leads to the saddle-point system
%
\begin{align*}
\begin{bmatrix}
(A + r\transpose{B}M^{-1}B) & \transpose{B}\\
B & 0
\end{bmatrix}
\begin{bmatrix}
x\\y
\end{bmatrix}
=
\begin{bmatrix}
b+r\transpose{B}M^{-1}c\\c
\end{bmatrix}
\end{align*}
%
The first diagonal equals the one of the penalty method for $r=1/\eps$, but the method is always conforming in the sense that the constraints are satisfied exactly, which allows for arbitrary choice of $r$.
%
If we combine with withe kernel method, we get
%
\begin{align*}
\begin{bmatrix}
A_{00} & A_{01} & 0\\
0 & B_1 & 0\\
A_{10} & (A_{11}+r\transpose{B}_1M^{-1}B_1) & \transpose{B}_1\\
\end{bmatrix}
\begin{bmatrix}
x_0\\x_1\\y
\end{bmatrix}
=
\begin{bmatrix}
b_0\\c\\b_1+r\transpose{B}_1M^{-1}c
\end{bmatrix},
\end{align*}
%
which shows that only the computation of the multiplier $y$ is changed, without changing it. 

A different situation arrives in the special case $B = \begin{bmatrix}0&A_{11}\end{bmatrix}$ and $M=A_{11}$. Then we have
%
\begin{align*}
\begin{bmatrix}
A_{00} & A_{01} & 0\\
A_{10} & (1+r)A_{11} & A_{11}\\
0 & A_{11}& 0
\end{bmatrix}
\begin{bmatrix}
x_0\\x_1\\y
\end{bmatrix}
=
\begin{bmatrix}
b_0\\b_1+rA_{11} c\\c
\end{bmatrix}
\end{align*}
%


%
%==========================================
\printbibliography[title=References Section~\thesection]
%==========================================

