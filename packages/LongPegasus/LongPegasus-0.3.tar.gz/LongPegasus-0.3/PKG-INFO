Metadata-Version: 1.1
Name: LongPegasus
Version: 0.3
Summary: A Longer Version of Pegasus TF Model For Abstractive Summarization
Home-page: https://github.com/abhilash1910/LongPegasus
Author: ABHILASH MAJUMDER
Author-email: debabhi1396@gmail.com
License: MIT
Download-URL: https://github.com/abhilash1910/LongPegasus/archive/v_03.tar.gz
Description: This package is used for inducing longformer self attention over base pegasus abstractive summarization model to increase the token limit and performance.The Pegasus is a large Transformer-based encoder-decoder model with a new pre-training objective which is adapted to abstractive summarization. More specifically, the pre-training objective, called "Gap Sentence Generation (GSG)", consists of masking important sentences from a document and generating these gap-sentences.On the other hand, the Longformer is a Transformer which replaces the full-attention mechanism (quadratic dependency) with a novel attention mechanism which scale linearly with the input sequence length. Consequently, Longformer can process sequences up to 4,096 tokens long (8 times longer than BERT which is limited to 512 tokens).This package plugs Longformers attention mechanism to Pegasus in order to perform abstractive summarization on long documents. The base modules are built on Tensorflow platform.
Keywords: Longformer,Self Attention,Global Attention,Gap sentence generation,Pegasus,Transformer,Encoder Decoder,Tensorflow
Platform: UNKNOWN
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Build Tools
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
